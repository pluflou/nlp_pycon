{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notdone_Part 4 - Applying modern deep learning to NLP",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pluflou/nlp_pycon/blob/master/Notdone_Part_4_Applying_modern_deep_learning_to_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIGjBVxfs_m",
        "colab_type": "text"
      },
      "source": [
        "## Part 4 - Applying Modern Deep Learning to NLP\n",
        "\n",
        "**Total time: 60 minutes**\n",
        "\n",
        "This notebook corresponds to the main block of the tutorial where we will cover how to build and train deep learning architectures, such as RNN, for an NLP task. The task at hand is emotion classification which is a multi-class problem. Let's dive in!\n",
        "\n",
        "---\n",
        "\n",
        "### Journey\n",
        "\n",
        "- Load the Data\n",
        "- Implementing Model\n",
        "- Pretesting Model\n",
        "- Setup Training\n",
        "- Traing Model\n",
        "- Storing Model\n",
        "- **Exercise:** Implementing Your Own Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6mdHUdLdHTf",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data\n",
        "Instead of reloading the data, we restore it from the previous phase.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBLZ0sFdE9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# helper functions\n",
        "def convert_to_pickle(item, directory):\n",
        "    pickle.dump(item, open(directory,\"wb\"))\n",
        "\n",
        "\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1pzre8l6-u",
        "colab_type": "code",
        "outputId": "90a39353-9387-4d7c-999b-9fe3b51b962c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# READ YOUR DATA FROM GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKt8eof-IqgA",
        "colab_type": "code",
        "outputId": "49e98828-61bc-4b1f-96e5-00627d7ebd36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# data instance\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x, y, x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "data_folder = \"/gdrive/My Drive/pycon2019/\"\n",
        "\n",
        "train_dataset = load_from_pickle(data_folder + \"train_dataset\")\n",
        "test_dataset = load_from_pickle(data_folder + \"test_dataset\")\n",
        "val_dataset = load_from_pickle(data_folder + \"val_dataset\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtHq06XGJkV4",
        "colab_type": "code",
        "outputId": "3ce1776f-26f2-465a-df67-2625f62d528b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dataset.batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP3VrH2h5Q7C",
        "colab_type": "text"
      },
      "source": [
        "### Implementing Model\n",
        "\n",
        "After the data has been preprocessed, transformed and prepared it is now time to construct the model or the so-called computation graph that will be used to train our classification models. We are going to use a gated recurrent neural network (GRU), which is considered a more efficient version of a basic RNN. The figure below shows a high-level overview of the model details. \n",
        "\n",
        "![alt txt](https://github.com/omarsar/nlp_pytorch_tensorflow_notebooks/blob/master/img/gru-model.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXwdHHeQfugz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmoGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # layers\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
        "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "    \n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.hidden_units)).to(device)\n",
        "    \n",
        "    def forward(self, x, lens, device):\n",
        "        x = self.embedding(x)\n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        output, self.hidden = self.gru(x, self.hidden) # max_len X batch_size X hidden_units\n",
        "        out = output[-1, :, :] \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out, self.hidden "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzcFnWB07Ev2",
        "colab_type": "text"
      },
      "source": [
        "### Pretesting models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqSmsXuoKDM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "TRAIN_BUFFER_SIZE = 40000 # len(input_tensor_train)\n",
        "VAL_BUFFER_SIZE = 5000 # len(input_tensor_val)\n",
        "TEST_BUFFER_SIZE = 5000 # len(input_tensor_test)\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
        "VAL_N_BATCH = VAL_BUFFER_SIZE // BATCH_SIZE\n",
        "TEST_N_BATCH = TEST_BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = 27291 # len(inputs.word2idx)\n",
        "target_size = 6 # num_emotions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CekF-k7m7F_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort batch function to be able to use with pad_packed_sequence\n",
        "# batch elements ordered decreasingle by their length\n",
        "\n",
        "def sort_batch(X, y, lengths):\n",
        "    \"sort the batch by length\"\n",
        "    \n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFmJQKBzdn3F",
        "colab_type": "text"
      },
      "source": [
        "`pad_packed_sequence` is a utility function to efficiently and automatically pad your data of variable length sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WziHVOpG7Lcz",
        "colab_type": "code",
        "outputId": "95cade4a-08d8-4968-fafb-05a6cd0fbfe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "model.to(device)\n",
        "\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(train_dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "print(\"Input size: \", xs.size())\n",
        "\n",
        "output, _ = model(xs.to(device), lens, device)\n",
        "print(output.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input size:  torch.Size([69, 64])\n",
            "torch.Size([64, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUeoDIDX7Ovk",
        "colab_type": "text"
      },
      "source": [
        "### Setup Training\n",
        "Now that we have tested the model, it is time to train it. We will define out optimization algorithm, learnin rate, and other necessary information to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuPVVgw7SZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enabling cuda\n",
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "model.to(device)\n",
        "\n",
        "# loss criterion and optimizer for training\n",
        "criterion = nn.CrossEntropyLoss() # the same as log_softmax + NLLLoss\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "def loss_function(y, prediction):\n",
        "    \"\"\" CrossEntropyLoss expects outputs and class indices as target \"\"\"\n",
        "    # convert from one-hot encoding to class indices\n",
        "    target = torch.max(y, 1)[1]\n",
        "    loss = criterion(prediction, target) \n",
        "    return loss   #TODO: refer the parameter of these functions as the same\n",
        "    \n",
        "def accuracy(target, logit):\n",
        "    ''' Obtain accuracy for training round '''\n",
        "    target = torch.max(target, 1)[1] # convert from one-hot encoding to class indices\n",
        "    corrects = (torch.max(logit, 1)[1].data == target).sum()\n",
        "    accuracy = 100.0 * corrects / len(logit)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc3tkiqQ9JzH",
        "colab_type": "text"
      },
      "source": [
        "### Training Model\n",
        "\n",
        "Now we finally train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujawKs7x9Lc8",
        "colab_type": "code",
        "outputId": "3e6d36ad-ee88-4b91-b696-2a166b35694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1777
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    ### Initialize hidden state\n",
        "    # TODO: do initialization here.\n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ, lens)) in enumerate(train_dataset):\n",
        "        loss = 0\n",
        "        predictions, _ = model(inp.permute(1 ,0).to(device), lens, device) # TODO:don't need _   \n",
        "              \n",
        "        loss += loss_function(targ.to(device), predictions)\n",
        "        batch_loss = (loss / int(targ.shape[1]))        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Val. Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.cpu().detach().numpy()))\n",
        "            \n",
        "    ### Validating\n",
        "    for (batch, (inp, targ, lens)) in enumerate(val_dataset):        \n",
        "        predictions,_ = model(inp.permute(1, 0).to(device), lens, device)        \n",
        "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             total_loss / TRAIN_N_BATCH, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / VAL_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Val. Loss 0.2962\n",
            "Epoch 1 Batch 100 Val. Loss 0.3004\n",
            "Epoch 1 Batch 200 Val. Loss 0.2443\n",
            "Epoch 1 Batch 300 Val. Loss 0.1161\n",
            "Epoch 1 Batch 400 Val. Loss 0.0412\n",
            "Epoch 1 Batch 500 Val. Loss 0.0377\n",
            "Epoch 1 Batch 600 Val. Loss 0.0309\n",
            "Epoch 1 Loss 0.1403 -- Train Acc. 66.0000 -- Val Acc. 90.0000\n",
            "Time taken for 1 epoch 24.551841259002686 sec\n",
            "\n",
            "Epoch 2 Batch 0 Val. Loss 0.0315\n",
            "Epoch 2 Batch 100 Val. Loss 0.0213\n",
            "Epoch 2 Batch 200 Val. Loss 0.0247\n",
            "Epoch 2 Batch 300 Val. Loss 0.0355\n",
            "Epoch 2 Batch 400 Val. Loss 0.0207\n",
            "Epoch 2 Batch 500 Val. Loss 0.0079\n",
            "Epoch 2 Batch 600 Val. Loss 0.0340\n",
            "Epoch 2 Loss 0.0260 -- Train Acc. 92.0000 -- Val Acc. 91.0000\n",
            "Time taken for 1 epoch 26.011555910110474 sec\n",
            "\n",
            "Epoch 3 Batch 0 Val. Loss 0.0328\n",
            "Epoch 3 Batch 100 Val. Loss 0.0066\n",
            "Epoch 3 Batch 200 Val. Loss 0.0141\n",
            "Epoch 3 Batch 300 Val. Loss 0.0341\n",
            "Epoch 3 Batch 400 Val. Loss 0.0217\n",
            "Epoch 3 Batch 500 Val. Loss 0.0190\n",
            "Epoch 3 Batch 600 Val. Loss 0.0136\n",
            "Epoch 3 Loss 0.0192 -- Train Acc. 93.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.501911640167236 sec\n",
            "\n",
            "Epoch 4 Batch 0 Val. Loss 0.0119\n",
            "Epoch 4 Batch 100 Val. Loss 0.0071\n",
            "Epoch 4 Batch 200 Val. Loss 0.0174\n",
            "Epoch 4 Batch 300 Val. Loss 0.0130\n",
            "Epoch 4 Batch 400 Val. Loss 0.0208\n",
            "Epoch 4 Batch 500 Val. Loss 0.0109\n",
            "Epoch 4 Batch 600 Val. Loss 0.0224\n",
            "Epoch 4 Loss 0.0170 -- Train Acc. 94.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.56792950630188 sec\n",
            "\n",
            "Epoch 5 Batch 0 Val. Loss 0.0071\n",
            "Epoch 5 Batch 100 Val. Loss 0.0261\n",
            "Epoch 5 Batch 200 Val. Loss 0.0148\n",
            "Epoch 5 Batch 300 Val. Loss 0.0182\n",
            "Epoch 5 Batch 400 Val. Loss 0.0017\n",
            "Epoch 5 Batch 500 Val. Loss 0.0147\n",
            "Epoch 5 Batch 600 Val. Loss 0.0135\n",
            "Epoch 5 Loss 0.0165 -- Train Acc. 95.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.559290170669556 sec\n",
            "\n",
            "Epoch 6 Batch 0 Val. Loss 0.0167\n",
            "Epoch 6 Batch 100 Val. Loss 0.0187\n",
            "Epoch 6 Batch 200 Val. Loss 0.0143\n",
            "Epoch 6 Batch 300 Val. Loss 0.0063\n",
            "Epoch 6 Batch 400 Val. Loss 0.0104\n",
            "Epoch 6 Batch 500 Val. Loss 0.0037\n",
            "Epoch 6 Batch 600 Val. Loss 0.0071\n",
            "Epoch 6 Loss 0.0137 -- Train Acc. 95.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.45026683807373 sec\n",
            "\n",
            "Epoch 7 Batch 0 Val. Loss 0.0033\n",
            "Epoch 7 Batch 100 Val. Loss 0.0080\n",
            "Epoch 7 Batch 200 Val. Loss 0.0084\n",
            "Epoch 7 Batch 300 Val. Loss 0.0017\n",
            "Epoch 7 Batch 400 Val. Loss 0.0189\n",
            "Epoch 7 Batch 500 Val. Loss 0.0160\n",
            "Epoch 7 Batch 600 Val. Loss 0.0185\n",
            "Epoch 7 Loss 0.0120 -- Train Acc. 96.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.47150182723999 sec\n",
            "\n",
            "Epoch 8 Batch 0 Val. Loss 0.0041\n",
            "Epoch 8 Batch 100 Val. Loss 0.0149\n",
            "Epoch 8 Batch 200 Val. Loss 0.0029\n",
            "Epoch 8 Batch 300 Val. Loss 0.0118\n",
            "Epoch 8 Batch 400 Val. Loss 0.0008\n",
            "Epoch 8 Batch 500 Val. Loss 0.0086\n",
            "Epoch 8 Batch 600 Val. Loss 0.0211\n",
            "Epoch 8 Loss 0.0105 -- Train Acc. 97.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.54435110092163 sec\n",
            "\n",
            "Epoch 9 Batch 0 Val. Loss 0.0003\n",
            "Epoch 9 Batch 100 Val. Loss 0.0046\n",
            "Epoch 9 Batch 200 Val. Loss 0.0154\n",
            "Epoch 9 Batch 300 Val. Loss 0.0170\n",
            "Epoch 9 Batch 400 Val. Loss 0.0044\n",
            "Epoch 9 Batch 500 Val. Loss 0.0038\n",
            "Epoch 9 Batch 600 Val. Loss 0.0193\n",
            "Epoch 9 Loss 0.0090 -- Train Acc. 97.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.487608671188354 sec\n",
            "\n",
            "Epoch 10 Batch 0 Val. Loss 0.0176\n",
            "Epoch 10 Batch 100 Val. Loss 0.0114\n",
            "Epoch 10 Batch 200 Val. Loss 0.0036\n",
            "Epoch 10 Batch 300 Val. Loss 0.0140\n",
            "Epoch 10 Batch 400 Val. Loss 0.0078\n",
            "Epoch 10 Batch 500 Val. Loss 0.0056\n",
            "Epoch 10 Batch 600 Val. Loss 0.0058\n",
            "Epoch 10 Loss 0.0081 -- Train Acc. 97.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 26.48466396331787 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAM43gri7LJ",
        "colab_type": "text"
      },
      "source": [
        "### Stopping the Model\n",
        "\n",
        "How do we know when to stop the model. We can use a technique called `early stopping`, not covered here, but widely used in deep learning, to control the convergence of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7w1P3PX9Qjq",
        "colab_type": "text"
      },
      "source": [
        "### Store the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvcWh9S9PHQ",
        "colab_type": "code",
        "outputId": "43365818-85ac-44bc-b279-2711c825a15d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.save(model, \"/gdrive/My Drive/pycon2019/emogru\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type EmoGRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cB1oo9_cnN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### GROUP EXERCISE (6-10 students) - Implementing your deep learning model\n",
        "Implement a model similar to the one above. Try to use an LSTM instead of an GRU. Go into the pytorch documentation and research quick ways to improve the model, like adding a `Dropout` [layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html). Anything to make your model faster and better. Also, add additional layers (i.e., make it deeper) to improve the model potential.\n",
        "\n",
        "Then you can share and present to the class and we will listen to what everyone learned and built. \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na4WVcdF_xCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nrQaqmceV6V",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "- [Emotion Recognition with PyTorch](https://github.com/omarsar/emotion_recognition_pytorch/blob/master/Deep_Learning_Emotion_Recognition_PyTorch.ipynb)\n",
        "\n",
        "- [Serialization Semantics by PyTorch](https://pytorch.org/docs/master/notes/serialization.html#recommend-saving-models)\n",
        "\n",
        "- [Word embeddings in PyTorch](https://github.com/omarsar/phd_2017/blob/master/pytorch_word_embeddings.ipynb)\n"
      ]
    }
  ]
}